# Use Ubuntu as base image
FROM ubuntu:20.04

# Set non-interactive frontend to avoid prompts during installation
ENV DEBIAN_FRONTEND=noninteractive

# Install required packages
RUN apt update && apt install -y \
    default-jdk \
    ssh \
    vim \
    r-base \
    python3 \
    openssh-server

# Configure SSH without password prompt
RUN ssh-keygen -t rsa -N "" -f $HOME/.ssh/id_rsa && \
    cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys && \
    chmod 0600 $HOME/.ssh/authorized_keys && \
    echo "export PDSH_RCMD_TYPE=ssh" >> ~/.bashrc

# Copy Hadoop tar.gz and package list
COPY hadoop.tar.gz /tmp/

# Install all packages from packagelist.txt

# Extract Hadoop
RUN mkdir /usr/local/hadoop && \
    tar -xzf /tmp/hadoop.tar.gz -C /usr/local/hadoop --strip-components=1 && \
    echo 'export HADOOP_HOME=/usr/local/hadoop' >> ~/.bashrc && \
    echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> ~/.bashrc

# Configure Hadoop environment
RUN sed -i 's|export JAVA_HOME=.*|export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64|' /usr/local/hadoop/etc/hadoop/hadoop-env.sh

# Configure core-site.xml
RUN sed -i '/<configuration>/a \ \ <property>\n\ \ \ <name>fs.defaultFS</name>\n\ \ \ <value>hdfs://172.18.0.2:9000</value>\n\ \ </property>' /usr/local/hadoop/etc/hadoop/core-site.xml

# Configure hdfs-site.xml
RUN sed -i '/<configuration>/a \ \ <property>\n\ \ \ <name>dfs.replication</name>\n\ \ \ <value>3</value>\n\ \ </property>\n\ \ <property>\n\ \ \ <name>dfs.namenode.name.dir</name>\n\ \ \ <value>file:/usr/local/hadoop_space/hdfs/namenode</value>\n\ \ </property>\n\ \ <property>\n\ \ \ <name>dfs.datanode.data.dir</name>\n\ \ \ <value>file:/usr/local/hadoop_space/hdfs/datanode</value>\n\ \ </property>\ \ <property>\n\ \ \ <name>dfs.namenode.rpc-address</name>\n\ \ \ <value>0.0.0.0:9000</value>\n\ \ </property>\ \ <property>\n\ \ \ <name>dfs.namenode.http-address</name>\n\ \ \ <value>0.0.0.0:50070</value>\n\ \ </property>\ \ <property>\n\ \ \ <name>dfs.datanode.address</name>\n\ \ \ <value>0.0.0.0:9866</value>\n\ \ </property>\ \ <property>\n\ \ \ <name>dfs.datanode.http-address</name>\n\ \ \ <value>0.0.0.0:9864</value>\n\ \ </property>' /usr/local/hadoop/etc/hadoop/hdfs-site.xml

# Configure yarn-site.xml
RUN sed -i '/<configuration>/a \ \ <property>\n\ \ \ <name>yarn.nodemanager.aux-services</name>\n\ \ \ <value>mapreduce_shuffle</value>\n\ \ </property>\n\ \ <property>\n\ \ \ <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n\ \ \ <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n\ \ </property>' /usr/local/hadoop/etc/hadoop/yarn-site.xml

# Configure mapred-site.xml
RUN sed -i '/<configuration>/a \ \ <property>\n\ \ \ <name>mapreduce.framework.name</name>\n\ \ \ <value>yarn</value>\n\ \ </property>\n\ \ <property>\n\ \ \ <name>yarn.app.mapreduce.am.env</name>\n\ \ \ <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n\ \ </property>\n\ \ <property>\n\ \ \ <name>mapreduce.map.env</name>\n\ \ \ <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n\ \ </property>\n\ \ <property>\n\ \ \ <name>mapreduce.reduce.env</name>\n\ \ \ <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n\ \ </property>' /usr/local/hadoop/etc/hadoop/mapred-site.xml

# Set environment variables
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 \
    HADOOP_HOME=/usr/local/hadoop \
    PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin \
    HADOOP_MAPRED_HOME=$HADOOP_HOME \
    HADOOP_COMMON_HOME=$HADOOP_HOME \
    HADOOP_HDFS_HOME=$HADOOP_HOME \
    YARN_HOME=$HADOOP_HOME \
    HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/native \
    HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/native" \
    HADOOP_HOME=/usr/local/hadoop \
    HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native \
    PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH \
    HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
    

# Create Hadoop directories
RUN mkdir -p /usr/local/hadoop_space/hdfs/namenode && \
    mkdir -p /usr/local/hadoop_space/hdfs/datanode && \
    hdfs namenode -format || true 


# Configure SSH
RUN echo "export PDSH_RCMD_TYPE=ssh" >> ~/.bashrc && \
    cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys
# Create a non-root user for Hadoop
RUN useradd -m -s /bin/bash hdfs && \
    echo "hdfs:hdfs" | chpasswd && \
    usermod -aG sudo hdfs


# Set Hadoop environment variables for the hdfs user
RUN echo 'export HADOOP_HOME=/usr/local/hadoop' >> /home/hdfs/.bashrc && \
    echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> /home/hdfs/.bashrc && \
    echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> /home/hdfs/.bashrc && \
    echo 'export HDFS_NAMENODE_USER=hdfs' >> /home/hdfs/.bashrc && \
    echo 'export HDFS_DATANODE_USER=hdfs' >> /home/hdfs/.bashrc && \
    echo 'export HDFS_SECONDARYNAMENODE_USER=hdfs' >> /home/hdfs/.bashrc

    
ENV HDFS_NAMENODE_USER=hdfs \
    HDFS_DATANODE_USER=hdfs \
    HDFS_SECONDARYNAMENODE_USER=hdfs    
# Configure SSH for the hdfs user
RUN mkdir -p /home/hdfs/.ssh && \
    ssh-keygen -t rsa -N "" -f /home/hdfs/.ssh/id_rsa && \
    cat /home/hdfs/.ssh/id_rsa.pub >> /home/hdfs/.ssh/authorized_keys && \
    chmod 0600 /home/hdfs/.ssh/authorized_keys && \
    chown -R hdfs:hdfs /home/hdfs/.ssh
    
RUN echo "source ~/.bashrc" >> /home/hdfs/.bash_profile

RUN echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> /home/hdfs/.bashrc && \
    echo 'export PATH=$PATH:$JAVA_HOME/bin' >> /home/hdfs/.bashrc


ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 \
    HADOOP_HOME=/usr/local/hadoop \
    PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin \
    HADOOP_MAPRED_HOME=$HADOOP_HOME \
    HADOOP_COMMON_HOME=$HADOOP_HOME \
    HADOOP_HDFS_HOME=$HADOOP_HOME \
    YARN_HOME=$HADOOP_HOME \
    HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/native \
    HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/native" \
    HADOOP_HOME=/usr/local/hadoop \
    HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native \
    PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH \
    HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native" \
    HDFS_NAMENODE_USER=root \
    HDFS_DATANODE_USER=root \
    HDFS_SECONDARYNAMENODE_USER=root
# RUN chown -R hdfs:hdfs /usr/local/hadoop /usr/local/hadoop_space
# RUN echo '#!/bin/bash' > /start-hdfs.sh && \
#     echo 'source /home/hdfs/.bashrc' >> /start-hdfs.sh && \
#     echo 'su - root -c "/usr/local/hadoop/sbin/start-dfs.sh"' >> /start-hdfs.sh && \
#     chmod +x /start-hdfs.sh


RUN apt-get update && apt-get install -y sudo && \
    echo 'user ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers


RUN echo "PermitRootLogin yes" >> /etc/ssh/sshd_config && \
    echo "PasswordAuthentication no" >> /etc/ssh/sshd_config && \
    echo "ChallengeResponseAuthentication no" >> /etc/ssh/sshd_config && \
    echo "UsePAM no" >> /etc/ssh/sshd_config

# Create logs directory and change ownership
RUN mkdir -p /usr/local/hadoop/logs && \
    chown -R hdfs:hdfs /usr/local/hadoop/logs && \
    chown -R hdfs:hdfs /usr/local/hadoop_space/hdfs && \
    chmod -R 755 /usr/local/hadoop_space/hdfs

RUN apt-get update && \
    apt-get install -y \
    gedit \                
    net-tools \            
    iputils-ping && \     
    apt-get clean && \     
    rm -rf /var/lib/apt/lists/*

RUN mkdir /var/run/sshd
RUN sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config
 
RUN echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh

RUN echo "hdfs ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers

USER hdfs



    
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 \
HADOOP_HOME=/usr/local/hadoop \
PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin \
HADOOP_MAPRED_HOME=$HADOOP_HOME \
HADOOP_COMMON_HOME=$HADOOP_HOME \
HADOOP_HDFS_HOME=$HADOOP_HOME \
YARN_HOME=$HADOOP_HOME \
HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/native \
HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/native" \
HADOOP_HOME=/usr/local/hadoop \
HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native \
PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH \
HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native" \
HDFS_NAMENODE_USER=hdfs \
HDFS_DATANODE_USER=hdfs \
HDFS_SECONDARYNAMENODE_USER=hdfs





EXPOSE 22


# Start SSH service
CMD service ssh start && bash
